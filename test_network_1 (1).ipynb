{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDlAypeXiweT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J0YKCUBi19u"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "def dot(x, y):\n",
        "    if isinstance(x[0],(int,float)):\n",
        "        if not isinstance(y[0],(int,float)) or len(x)!=len(y):\n",
        "            raise ValueError(\"dimensions don't match\")\n",
        "        ans = 0\n",
        "        for i in range(len(x)):\n",
        "            ans+=x[i]*y[i]\n",
        "        return ans\n",
        "    if isinstance(y[0],(int,float)):\n",
        "        if not isinstance(x[0],(int,float)) or len(x)!=len(y):\n",
        "            raise ValueError(\"dimensions don't match\")\n",
        "    if len(x[0]) != len(y):\n",
        "        raise ValueError(\"Number of columns in matrix x must equal number of rows in matrix y.\")\n",
        "\n",
        "    rows_x, cols_x = len(x), len(x[0])\n",
        "    rows_y, cols_y = len(y), len(y[0])\n",
        "\n",
        "    result = [[0 for _ in range(cols_y)] for _ in range(rows_x)]\n",
        "    for i in range(rows_x):\n",
        "        for j in range(cols_y):\n",
        "            for k in range(cols_x):\n",
        "                result[i][j] += x[i][k] * y[k][j]\n",
        "    return result\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Activation function for sigmoid is f(x) = 1/(1+e^(-x))\n",
        "    if isinstance(x,(int,float)):\n",
        "        return 1/(1+math.exp(-x))\n",
        "\n",
        "    if isinstance(x,list):\n",
        "        return [sigmoid(elem) for elem in x]\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU activation function.\n",
        "    f(x) = max(0, x)\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return max(0, x)\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [relu(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [relu(row) for row in x]\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Leaky ReLU activation function.\n",
        "    f(x) = max(alpha * x, x)\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return max(alpha * x, x)\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [leaky_relu(elem, alpha) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [leaky_relu(row, alpha) for row in x]\n",
        "\n",
        "def elu(x, alpha=1.0):\n",
        "    \"\"\"\n",
        "    ELU activation function.\n",
        "    f(x) = x for x > 0, alpha * (exp(x) - 1) for x <= 0\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return x if x > 0 else alpha * (math.exp(x) - 1)\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [elu(elem, alpha) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [elu(row, alpha) for row in x]\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Tanh activation function.\n",
        "    f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return math.tanh(x)\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [tanh(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [tanh(row) for row in x]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax activation function (only works on lists or 2D matrices).\n",
        "    f(x)_i = exp(x_i) / sum(exp(x_j) for all j)\n",
        "    \"\"\"\n",
        "    if isinstance(x, list):\n",
        "        exp_x = [math.exp(i) for i in x]\n",
        "        total = sum(exp_x)\n",
        "        return [i / total for i in exp_x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [softmax(row) for row in x]\n",
        "\n",
        "def swish(x):\n",
        "    \"\"\"\n",
        "    Swish activation function.\n",
        "    f(x) = x * sigmoid(x)\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return x * (1 / (1 + math.exp(-x)))\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [swish(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [swish(row) for row in x]\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    GELU activation function.\n",
        "    f(x) = 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [gelu(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [gelu(row) for row in x]\n",
        "\n",
        "def softplus(x):\n",
        "    \"\"\"\n",
        "    Softplus activation function.\n",
        "    f(x) = ln(1 + exp(x))\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return math.log(1 + math.exp(x))\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [softplus(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [softplus(row) for row in x]\n",
        "\n",
        "def hard_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Hard Sigmoid activation function (simplified version of sigmoid).\n",
        "    f(x) = clip((x + 1) / 2, 0, 1)\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return min(max((x + 1) / 2, 0), 1)\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [hard_sigmoid(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [hard_sigmoid(row) for row in x]\n",
        "\n",
        "def hard_swish(x):\n",
        "    \"\"\"\n",
        "    Hard Swish activation function.\n",
        "    f(x) = x * clip(x + 3, 0, 6) / 6\n",
        "    \"\"\"\n",
        "    if isinstance(x, (int, float)):\n",
        "        return x * min(max(x + 3, 0), 6) / 6\n",
        "\n",
        "    if isinstance(x, list):\n",
        "        return [hard_swish(elem) for elem in x]\n",
        "\n",
        "    if isinstance(x, list) and isinstance(x[0], list):\n",
        "        return [hard_swish(row) for row in x]\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    if isinstance(x,(int,float)):\n",
        "        s = sigmoid(x)\n",
        "        return s * (1-s)\n",
        "    if isinstance(x,list):\n",
        "        return [sigmoid_derivative(elem) for elem in x]\n",
        "\n",
        "    raise ValueError(\"Unsupported type for sigmoid_derivative\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk5iw2lgi8Ko"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-15  # to avoid log(0)\n",
        "    y_pred = [max(min(yp, 1 - epsilon), epsilon) for yp in y_pred]\n",
        "    return -sum(yt * math.log(yp) + (1 - yt) * math.log(1 - yp) for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "# Example usage:\n",
        "y_true = [1, 0, 1, 1]\n",
        "y_pred = [0.9, 0.1, 0.8, 0.7]\n",
        "\n",
        "#print(\"MSE:\", mean_squared_error(y_true, y_pred))\n",
        "#print(\"Cross-Entropy Loss:\", cross_entropy_loss(y_true, y_pred))\n",
        "\n",
        "\n",
        "def mean_squared_error_derivative(y_true, y_pred):\n",
        "    return [(2 / len(y_true)) * (yp - yt) for yt, yp in zip(y_true, y_pred)]\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-15  # to avoid log(0)\n",
        "    y_pred = [max(min(yp, 1 - epsilon), epsilon) for yp in y_pred]\n",
        "    return -sum(yt * math.log(yp) + (1 - yt) * math.log(1 - yp) for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "def cross_entropy_loss_derivative(y_true, y_pred):\n",
        "    epsilon = 1e-15  # to avoid division by zero\n",
        "    y_pred = [max(min(yp, 1 - epsilon), epsilon) for yp in y_pred]\n",
        "    return [-(yt / yp) + (1 - yt) / (1 - yp) for yt, yp in zip(y_true, y_pred)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDOsRBM6i_T8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias, activation = sigmoid, activation_grad = sigmoid_derivative):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "        self.activation = activation\n",
        "        self.activation_grad = activation_grad\n",
        "    def feed_forward(self, inputs):\n",
        "        self.input_cache = inputs\n",
        "        total = dot(self.weights, inputs)\n",
        "        # If total is scalar, you directly add bias\n",
        "        if isinstance(total, (int, float)):\n",
        "            self.output = self.activation(total + self.bias)\n",
        "            return self.output\n",
        "\n",
        "        # If total is a list (or higher dimension), we must handle broadcasting and apply bias element-wise\n",
        "        if isinstance(total, list):\n",
        "            self.output = self.activation(self.apply_bias_elementwise(total,self.bias))\n",
        "            return self.output\n",
        "\n",
        "    def apply_bias_elementwise(self, total, bias):\n",
        "        # For element-wise addition of bias based on the dimensionality of total and bias\n",
        "        if isinstance(total, list):\n",
        "            if isinstance(total[0], (list, tuple)):  # Handle case for 2D or higher\n",
        "                return [self.apply_bias_elementwise(row, bias) for row in total]\n",
        "            else:  # Handle 1D case (list of scalars)\n",
        "                if isinstance(bias, (list, tuple)):  # Bias should be of the same length as total for 1D case\n",
        "                    return [total[i] + bias[i] for i in range(len(total))]\n",
        "                else:  # Scalar bias\n",
        "                    return [elem + bias for elem in total]\n",
        "\n",
        "        # This would handle scalar `total`, but we expect to handle list or higher only.\n",
        "        raise ValueError(\"The total should be a list, tuple, or higher dimensional structure.\")\n",
        "            # assume it is a list here\n",
        "\n",
        "    def multiply_element_wise(self, a,b):\n",
        "        if isinstance(a,(int,float)) and isinstance(b,(int,float)):\n",
        "            return a*b\n",
        "        return [self.multiply_element_wise(a[i],b[i]) for i in range(len(a))]\n",
        "    def backward(self, grad):\n",
        "        # Compute gradient with respect to weights and bias\n",
        "        grad_activation = self.activation_grad(self.output)\n",
        "\n",
        "        grad_total = self.multiply_element_wise(grad, grad_activation)\n",
        "\n",
        "        self.grad_weights = [grad_total * inp for inp in self.input_cache]\n",
        "        self.grad_bias = grad_total\n",
        "\n",
        "        grad_inputs = [grad_total * w for w in self.weights]\n",
        "\n",
        "        return grad_inputs\n",
        "\n",
        "    def update_params(self, learning_rate = 0.01):\n",
        "        self.weights = [w - learning_rate * grad_w for w, grad_w in zip(self.weights, self.grad_weights)]\n",
        "        self.bias -= learning_rate * self.grad_bias\n",
        "\n",
        "\n",
        "class Dense:\n",
        "    def __init__(self, num_neurons, input_size, activation = sigmoid, activation_grad = sigmoid_derivative):\n",
        "        self.num_neurons = num_neurons\n",
        "        self.input_size = input_size\n",
        "        self.activation = activation\n",
        "        self.activation_grad = activation_grad\n",
        "        self.initialize_neurons()\n",
        "\n",
        "    def initialize_neurons(self):\n",
        "        self.neurons = []\n",
        "        scale = 1\n",
        "        for _ in range(self.num_neurons):\n",
        "            weights = [random.uniform(-scale,scale) for _ in range(self.input_size)]\n",
        "            bias = random.uniform(-scale,scale)\n",
        "            neuron = Neuron(weights, bias, self.activation, self.activation_grad)\n",
        "            self.neurons.append(neuron)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = [neuron.feed_forward(inputs) for neuron in self.neurons]\n",
        "        return output\n",
        "\n",
        "    def backward(self, grads):\n",
        "        grad_inputs = [0 for _ in range(self.input_size)]\n",
        "        for i, neuron in enumerate(self.neurons):\n",
        "            grad_input = neuron.backward(grads[i])\n",
        "            grad_inputs = [grad_inputs[j] + grad_input[j] for j in range(self.input_size)]\n",
        "        return grad_inputs\n",
        "\n",
        "    def update_params(self, learning_rate = 0.01):\n",
        "        for neuron in self.neurons:\n",
        "            neuron.update_params(learning_rate)\n",
        "\n",
        "\n",
        "class Conv2D:\n",
        "    def __init__(self, num_filters, filter_size, input_shape, stride = 1, padding = 'valid', activation_function = None):\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "        self.input_shape = input_shape\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.activation_function = None\n",
        "\n",
        "        self.initialize_filters()\n",
        "        self.biases = [random.random() for _ in range(self.num_filters)]\n",
        "\n",
        "    def initialize_filters(self):\n",
        "        self.filters = []\n",
        "        for _ in range(self.num_filters):\n",
        "            filter_mat = []\n",
        "            for _ in range(self.filter_size[0]):\n",
        "                filter_mat.append([[random.random() for _ in range(self.input_shape[2])] for _ in range(self.filter_size[1])])\n",
        "            self.filters.append(filter_mat)\n",
        "\n",
        "    def apply_padding(self,inputs):\n",
        "        if self.padding == 'same':\n",
        "            pad_height = (self.filter_size[0]-1)//2\n",
        "            pad_width = (self.filter_size[1]-1)//2\n",
        "            padded_input = []\n",
        "\n",
        "            for channel in range(len(inputs[0][0])):\n",
        "                padded_channel =[[0] * (len(inputs[0]) + 2*pad_width)\n",
        "                                 for _ in range(len(inputs)+2*pad_height)]\n",
        "                for i in range(len(inputs)):\n",
        "                    for j in range(len(inputs[0])):\n",
        "                        padded_channel[i+pad_height][j+pad_width]=inputs[i][j][channel]\n",
        "                    padded_input.append(padded_channel)\n",
        "            return padded_input\n",
        "        return inputs\n",
        "\n",
        "    def element_wise_product_sum(self, slice_input, filter, bias):\n",
        "        total = 0\n",
        "        for i in range(len(filter)):\n",
        "            for j in range(len(filter[0])):\n",
        "                for k in range(len(filter[0][0])):\n",
        "                    total += slice_input[i][j][k]*filter[i][j][k]\n",
        "        return total+bias\n",
        "\n",
        "    def convolve(self,inputs):\n",
        "        inputs = self.apply_padding(inputs)\n",
        "\n",
        "        input_height = len(inputs)\n",
        "        input_width = len(inputs[0])\n",
        "        input_depth = len(inputs[0][0])\n",
        "\n",
        "        output_height = (input_height-self.filter_size[0])//self.stride + 1\n",
        "        output_width = (input_width - self.filter_size[1])//self.stride + 1\n",
        "\n",
        "        output = [[[0.0 for _ in range(self.num_filters)] for _ in range(output_width)] for _ in range(output_height)]\n",
        "\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                for f in range(self.num_filters):\n",
        "                    vertical_start = i*self.stride\n",
        "                    vertical_end = vertical_start + self.filter_size[0]\n",
        "                    horizontal_start = j*self.stride\n",
        "                    horizontal_end = horizontal_start + self.filter_size[1]\n",
        "\n",
        "                    slice_input = [\n",
        "                        inputs[vertical_start+x][horizontal_start:horizontal_end]\n",
        "                        for x in range(self.filter_size[0])\n",
        "                    ]\n",
        "\n",
        "                    value = self.element_wise_product_sum(slice_input, self.filters[f], self.biases[f])\n",
        "\n",
        "                    if self.activation_function:\n",
        "                        value = self.activation_function(value)\n",
        "                output[i][j][f]=value\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXIw7BO5jKz1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = inputs\n",
        "        for layer in self.layers:\n",
        "            output = layer.convolve(output) if isinstance(layer, Conv2D) else layer.forward(output)\n",
        "        return output\n",
        "\n",
        "    def backward(self, loss_grad):\n",
        "        grad = loss_grad\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backward(grad)\n",
        "            #print(f'grad at layer is {grad}')\n",
        "\n",
        "    def update_params(self, learning_rate = 0.01):\n",
        "        for layer in self.layers:\n",
        "            layer.update_params(learning_rate)\n",
        "\n",
        "    def train(self, inputs, targets, loss_function, loss_grad_function, epochs, learning_rate = 0.01,):\n",
        "        for epoch in range(epochs):\n",
        "            print(f'epoch {epoch+1}/{epochs}')\n",
        "            loss = 0\n",
        "            #print(f'enumerating over data')\n",
        "            for i, input in enumerate(inputs):\n",
        "                target = targets[i]\n",
        "                #if i % 100 == 0:\n",
        "                    #print(f'before prediction on iteration {i}')\n",
        "                prediction = self.forward(input)\n",
        "                #if i % 100 == 0:\n",
        "                    #print(f'after prediction on iteration {i}')\n",
        "                loss += loss_function(target, prediction)\n",
        "                loss_grad = loss_grad_function(target, prediction)\n",
        "                #print(f'loss grad is {loss_grad}')\n",
        "                self.backward(loss_grad)\n",
        "            print(f'epoch {epoch+1}/{epochs}, loss: {loss/len(inputs)}')\n",
        "\n",
        "            self.update_params(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwWIe5wnjXi5"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reduce the size of the dataset\n",
        "# Choose a random subset of 3000 training samples and 500 test samples\n",
        "train_indices = np.random.choice(X_train.shape[0], 3000, replace=False)\n",
        "test_indices = np.random.choice(X_test.shape[0], 500, replace=False)\n",
        "\n",
        "X_train = X_train[train_indices].reshape(3000, -1) / 255.0  # Flatten and normalize\n",
        "X_test = X_test[test_indices].reshape(500, -1) / 255.0     # Flatten and normalize\n",
        "y_train = y_train[train_indices]\n",
        "y_test = y_test[test_indices]\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Split 3000 training samples into train and validation sets\n",
        "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(\n",
        "    X_train, y_train_encoded, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Convert datasets into lists if required\n",
        "X_train = list(X_train)\n",
        "X_val = list(X_val)\n",
        "X_test = list(X_test)\n",
        "y_train_encoded = list(y_train_encoded)\n",
        "y_val_encoded = list(y_val_encoded)\n",
        "y_test_encoded = list(y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHMdErWWjcIF",
        "outputId": "84fa486e-1479-4a79-d02a-ca24f395988d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2700\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(f'{len(X_train)}')\n",
        "print(f'{type(X_train)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVVlN3RZjfW2"
      },
      "outputs": [],
      "source": [
        "nn = NeuralNetwork()\n",
        "layer_one = Dense(num_neurons=128, input_size=784)\n",
        "layer_two = Dense(num_neurons=64, input_size=128)\n",
        "layer_three = Dense(num_neurons=10, input_size=64)\n",
        "\n",
        "nn.add_layer(layer_one)  # First hidden layer with sigmoid\n",
        "nn.add_layer(layer_two)   # Second hidden layer with sigmoid\n",
        "nn.add_layer(layer_three)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7PZQMVXjnPv",
        "outputId": "0cd37551-9a2e-4ea7-b4e8-9219e2dd7357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/100\n",
            "epoch 1/100, loss: 1.3230049212342592\n",
            "epoch 2/100\n",
            "epoch 2/100, loss: 0.6973699541972684\n",
            "epoch 3/100\n",
            "epoch 3/100, loss: 0.6694519772908976\n",
            "epoch 4/100\n",
            "epoch 4/100, loss: 0.655308278964443\n",
            "epoch 5/100\n",
            "epoch 5/100, loss: 0.6470332325151484\n",
            "epoch 6/100\n",
            "epoch 6/100, loss: 0.6422884217839918\n",
            "epoch 7/100\n",
            "epoch 7/100, loss: 0.6400223855233625\n",
            "epoch 8/100\n",
            "epoch 8/100, loss: 0.6396588876567321\n",
            "epoch 9/100\n",
            "epoch 9/100, loss: 0.6408403656045183\n",
            "epoch 10/100\n",
            "epoch 10/100, loss: 0.6433244887803393\n",
            "epoch 11/100\n",
            "epoch 11/100, loss: 0.6469356037390133\n",
            "epoch 12/100\n",
            "epoch 12/100, loss: 0.6515394255286452\n",
            "epoch 13/100\n",
            "epoch 13/100, loss: 0.6570288048012282\n",
            "epoch 14/100\n",
            "epoch 14/100, loss: 0.6633152540559477\n",
            "epoch 15/100\n",
            "epoch 15/100, loss: 0.670323676271772\n",
            "epoch 16/100\n",
            "epoch 16/100, loss: 0.6779889705610024\n",
            "epoch 17/100\n",
            "epoch 17/100, loss: 0.6862537842547518\n",
            "epoch 18/100\n",
            "epoch 18/100, loss: 0.6950669873116261\n",
            "epoch 19/100\n",
            "epoch 19/100, loss: 0.7043826122948462\n",
            "epoch 20/100\n",
            "epoch 20/100, loss: 0.7141590994545961\n",
            "epoch 21/100\n",
            "epoch 21/100, loss: 0.7243587441640057\n",
            "epoch 22/100\n",
            "epoch 22/100, loss: 0.7349472794940853\n",
            "epoch 23/100\n",
            "epoch 23/100, loss: 0.7458935491015516\n",
            "epoch 24/100\n",
            "epoch 24/100, loss: 0.7571692402338124\n",
            "epoch 25/100\n",
            "epoch 25/100, loss: 0.768748656794367\n",
            "epoch 26/100\n",
            "epoch 26/100, loss: 0.7806085198651014\n",
            "epoch 27/100\n",
            "epoch 27/100, loss: 0.7927277886185049\n",
            "epoch 28/100\n",
            "epoch 28/100, loss: 0.8050874983154573\n",
            "epoch 29/100\n",
            "epoch 29/100, loss: 0.8176706140164214\n",
            "epoch 30/100\n",
            "epoch 30/100, loss: 0.830461898861007\n",
            "epoch 31/100\n",
            "epoch 31/100, loss: 0.8434477948138558\n",
            "epoch 32/100\n",
            "epoch 32/100, loss: 0.8566163124785026\n",
            "epoch 33/100\n",
            "epoch 33/100, loss: 0.8699569257898614\n",
            "epoch 34/100\n",
            "epoch 34/100, loss: 0.8834604675875231\n",
            "epoch 35/100\n",
            "epoch 35/100, loss: 0.8971190232078186\n",
            "epoch 36/100\n",
            "epoch 36/100, loss: 0.9109258209037803\n",
            "epoch 37/100\n",
            "epoch 37/100, loss: 0.924875119616181\n",
            "epoch 38/100\n",
            "epoch 38/100, loss: 0.9389620960102129\n",
            "epoch 39/100\n",
            "epoch 39/100, loss: 0.9531827335657428\n",
            "epoch 40/100\n",
            "epoch 40/100, loss: 0.9675337167811907\n",
            "epoch 41/100\n",
            "epoch 41/100, loss: 0.9820123332064985\n",
            "epoch 42/100\n",
            "epoch 42/100, loss: 0.9966163851308686\n",
            "epoch 43/100\n",
            "epoch 43/100, loss: 1.011344111513421\n",
            "epoch 44/100\n",
            "epoch 44/100, loss: 1.0261941194696893\n",
            "epoch 45/100\n",
            "epoch 45/100, loss: 1.0411653236426932\n",
            "epoch 46/100\n",
            "epoch 46/100, loss: 1.056256891317268\n",
            "epoch 47/100\n",
            "epoch 47/100, loss: 1.0714681912228659\n",
            "epoch 48/100\n",
            "epoch 48/100, loss: 1.0867987444868914\n",
            "epoch 49/100\n",
            "epoch 49/100, loss: 1.102248176933052\n",
            "epoch 50/100\n",
            "epoch 50/100, loss: 1.11781617265265\n",
            "epoch 51/100\n",
            "epoch 51/100, loss: 1.1335024293547018\n",
            "epoch 52/100\n",
            "epoch 52/100, loss: 1.1493066163471373\n",
            "epoch 53/100\n",
            "epoch 53/100, loss: 1.1652283361178317\n",
            "epoch 54/100\n",
            "epoch 54/100, loss: 1.1812670904311613\n",
            "epoch 55/100\n",
            "epoch 55/100, loss: 1.1974222517181503\n",
            "epoch 56/100\n",
            "epoch 56/100, loss: 1.213693040388522\n",
            "epoch 57/100\n",
            "epoch 57/100, loss: 1.2300785085618062\n",
            "epoch 58/100\n",
            "epoch 58/100, loss: 1.2465775305915736\n",
            "epoch 59/100\n",
            "epoch 59/100, loss: 1.2631888006017615\n",
            "epoch 60/100\n",
            "epoch 60/100, loss: 1.2799108370267018\n",
            "epoch 61/100\n",
            "epoch 61/100, loss: 1.296741993825183\n",
            "epoch 62/100\n",
            "epoch 62/100, loss: 1.3136804776433504\n",
            "epoch 63/100\n",
            "epoch 63/100, loss: 1.330724369796141\n",
            "epoch 64/100\n",
            "epoch 64/100, loss: 1.3478716516137208\n",
            "epoch 65/100\n",
            "epoch 65/100, loss: 1.365120231534205\n",
            "epoch 66/100\n",
            "epoch 66/100, loss: 1.3824679723277478\n",
            "epoch 67/100\n",
            "epoch 67/100, loss: 1.3999127169498595\n",
            "epoch 68/100\n",
            "epoch 68/100, loss: 1.417452311670035\n",
            "epoch 69/100\n",
            "epoch 69/100, loss: 1.435084625315273\n",
            "epoch 70/100\n",
            "epoch 70/100, loss: 1.4528075638319948\n",
            "epoch 71/100\n",
            "epoch 71/100, loss: 1.470619080030197\n",
            "epoch 72/100\n",
            "epoch 72/100, loss: 1.4885171792471774\n",
            "epoch 73/100\n",
            "epoch 73/100, loss: 1.5064999223692825\n",
            "epoch 74/100\n",
            "epoch 74/100, loss: 1.5245654277037786\n",
            "epoch 75/100\n",
            "epoch 75/100, loss: 1.5427118724641045\n",
            "epoch 76/100\n",
            "epoch 76/100, loss: 1.560937493571035\n",
            "epoch 77/100\n",
            "epoch 77/100, loss: 1.579240586833818\n",
            "epoch 78/100\n",
            "epoch 78/100, loss: 1.597619503794131\n",
            "epoch 79/100\n"
          ]
        }
      ],
      "source": [
        "nn.train(\n",
        "    inputs = X_train,\n",
        "    targets = y_train_encoded,\n",
        "    loss_function = cross_entropy_loss,\n",
        "    loss_grad_function= cross_entropy_loss_derivative,\n",
        "    epochs = 100,\n",
        "    learning_rate = 0.01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XMyaxKJjn0k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}